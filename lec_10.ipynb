{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import required library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential, layers\n",
    "from tensorflow.keras.layers import Input,SimpleRNN\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.losses import MeanSquaredError as MSELoss\n",
    "from tensorflow.keras.metrics import MeanAbsolutePercentageError as MAPEMetrics\n",
    "from tensorflow.keras.metrics import MeanAbsoluteError as MAEMetrics\n",
    "from tensorflow.keras.metrics import MeanSquaredError as MSEMetrics\n",
    "from tensorflow.keras.metrics import R2Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load data from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/OPCUA_PM25_interpolated.csv', names=['Date', 'Temperature', 'Humidity', 'PM03', 'PM05', 'PM1', 'PM25', 'PM5', 'PM10'], header=0)\n",
    "df_idx = df.copy(deep=True)\n",
    "df_idx = df_idx.set_index(['Date'])\n",
    "df_idx.index = pd.to_datetime(df_idx.index)\n",
    "df_idx = df_idx.loc['2023-06-02 01:00:00':'2023-06-19 23:00:00']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Check data statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Data head: ', df_idx.head(), '\\nData tail: ', df_idx.tail(), '\\nData shape: ', df_idx.shape, '\\nData Describe: ', df_idx.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(num=None, figsize=(24, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(df_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Inspect each data features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspectData(data):\n",
    "    checkNull = data.isnull().any()\n",
    "    lengthData = len(data)\n",
    "    maxVal = data.max()\n",
    "    minVal = data.min()\n",
    "    varVal = data.var()\n",
    "    stdVal = data.std()\n",
    "    meanVal = data.mean()\n",
    "    medianVal = data.median()\n",
    "    print('Data nullities: ', checkNull, ' | Data length: ', lengthData)\n",
    "    print('Data max value: ', maxVal, ' | Data min value: ', minVal)\n",
    "    print('Data variance: ', varVal, ' | Data standard deviation: ', stdVal)\n",
    "    print('Data mean: ', meanVal, ' | Data median: ', medianVal)\n",
    "    plt.figure(num=None, figsize=(24, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "    plt.plot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df_idx:\n",
    "    print(f'[INFO] {column}')\n",
    "    inspectData(df_idx[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Detailed statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idx.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idx.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idx.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zScore = StandardScaler()\n",
    "np_alldata = zScore.fit_transform(df_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alldata = pd.DataFrame(np_alldata, columns = ['Temperature', 'Humidity', 'PM03', 'PM05', 'PM1', 'PM25', 'PM50', 'PM10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alldata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alldata.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df_alldata:\n",
    "    print(f'[INFO] {column}')\n",
    "    inspectData(df_alldata[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Inspect feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(15,9)})\n",
    "sns.heatmap(df_alldata.corr(),vmin=0,vmax=1,annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Drop unimportant feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alldata = df_alldata.drop(['Temperature', 'Humidity', 'PM03', 'PM10'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alldata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alldata.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Develop RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1. Specify training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_length = 0.2\n",
    "lr = 5e-8\n",
    "labels_length = 3660\n",
    "seq_length = 60\n",
    "data_features = 4\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "checkpoint_cb_monitor = 'val_loss'\n",
    "earlystop_cb_monitor = 'loss'\n",
    "earlystop_cb_minDelta = 0.000001\n",
    "earlystop_cb_patience = 100\n",
    "reduceLR_cb_monitor = 'val_loss'\n",
    "reduceLR_cb_factor = 0.95\n",
    "reduceLR_cb_patience = 100\n",
    "reduceLR_cb_minDelta = 0.000001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2. Split into train and val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int((len(df_alldata) - labels_length) * (1 - val_length))\n",
    "val_size = int((len(df_alldata) - labels_length) - train_size)\n",
    "print('\\nData length: ', len(df_alldata), '\\nTrain data size: ', train_size, '\\nVal data size: ', val_size)\n",
    "\n",
    "df_train = pd.DataFrame(df_alldata.iloc[0:train_size, :])\n",
    "df_valid = pd.DataFrame(df_alldata.iloc[train_size:train_size+val_size, :])\n",
    "df_test = pd.DataFrame(df_alldata.iloc[train_size+val_size:, :])\n",
    "\n",
    "print('\\nData length: ', len(df_train), '\\nData head: ', df_train.head(), '\\nData tail: ', df_train.tail())\n",
    "print('\\nData length: ', len(df_valid), '\\nData head: ', df_valid.head(), '\\nData tail: ', df_valid.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3. Convert to sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset (X, y, look_back = 1):\n",
    "    Xs, ys = [], []\n",
    " \n",
    "    for i in range(0,len(X)-look_back):\n",
    "        v = X[i:i+look_back]\n",
    "        w = y[i+look_back]\n",
    "        Xs.append(v)\n",
    "        ys.append(w)\n",
    " \n",
    "    return np.array(Xs), np.expand_dims(np.array(ys), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_train = df_train.to_numpy()\n",
    "np_valid = df_valid.to_numpy()\n",
    "np_test = df_test.to_numpy()\n",
    "\n",
    "X_train, y_train = create_dataset(np_train,np_train[:,2], seq_length)\n",
    "X_valid, y_valid = create_dataset(np_valid,np_valid[:,2], seq_length)\n",
    "X_test, y_test = create_dataset(np_test,np_test[:,2], seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4. Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(units):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(seq_length, data_features)))\n",
    "    model.add(SimpleRNN(units = units*2, return_sequences=True))\n",
    "    model.add(SimpleRNN(units = units*5, return_sequences=True))\n",
    "    model.add(SimpleRNN(units = 1))\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "                  loss='mse',\n",
    "                  metrics=[MAEMetrics(), MSEMetrics(), MAPEMetrics(), tf.nn.log_poisson_loss, R2Score])\n",
    "    return model\n",
    "\n",
    "model_build = create_model(seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_build.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.5. Define optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'model_store/best_model.weights.h5'\n",
    "\n",
    "cp_callback = ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "earlystop_callback = EarlyStopping(monitor='loss', min_delta=0.000001, patience=20)\n",
    "hist_callback = tf.keras.callbacks.History()\n",
    "reduceLR_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.005, patience=10, verbose=1, min_delta=0.000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.6. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model):\n",
    "    history = model.fit(X_train, y_train, epochs=epochs,  \n",
    "                        validation_data=(X_valid, y_valid),\n",
    "                        batch_size=batch_size, shuffle=True,\n",
    "                        callbacks=[cp_callback, hist_callback, earlystop_callback, reduceLR_callback])\n",
    "#                         callbacks=[cp_callback, hist_callback, reduceLR_callback])\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "st_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "st_time2 = datetime.now().strftime('%Y%m%d %H%M%S')\n",
    "print('Start time: ', datetime.now())\n",
    "\n",
    "train_history = fit_model(model_build)\n",
    "\n",
    "print('Start time: ', st_time, '\\nFinished time: ', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "print('Overall training time: ', (time.time()-start_time)/3600, ' hours')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.7. Plot training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss (history):\n",
    "    fig, ax= plt.subplots(figsize = (10, 6))\n",
    "    ax.set_xlim(0, len(history.history['loss'])-1)\n",
    "#     ax.set_ylim(0, 1)\n",
    "    ax.tick_params(axis=\"y\",direction=\"in\")\n",
    "    ax.tick_params(axis=\"x\",direction=\"in\")\n",
    "    ax.xaxis.set_major_locator(MultipleLocator(0.1*len(history.history['loss'])))\n",
    "#     ax.yaxis.set_major_locator(MultipleLocator(.2))\n",
    "    \n",
    "    plt.plot(history.history['loss'],linewidth=2)\n",
    "    plt.plot(history.history['val_loss'],linewidth=2)\n",
    "    csfont = {'fontname':'Times New Roman'}\n",
    "    plt.title('PM25 Prediction using RNN')\n",
    "    plt.ylabel('Loss',fontsize=14,**csfont)\n",
    "    plt.xlabel('epoch',fontsize=14,**csfont)\n",
    "    plt.legend(['Train loss', 'Validation loss'], loc='upper right',fontsize=12)\n",
    "\n",
    "plot_loss(train_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Test the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1. Load the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_build.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2. Perform the prediction using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_result = model_build.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3. Plot the prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_future(prediction, y_test):\n",
    "    fig, ax= plt.subplots(figsize = (12, 8))\n",
    "    ax.set_xlim(0, y_test.shape[0])\n",
    "    ax.tick_params(axis=\"y\",direction=\"in\")\n",
    "    ax.tick_params(axis=\"x\",direction=\"in\")\n",
    "    ax.xaxis.set_major_locator(MultipleLocator(0.1*y_test.shape[0]))\n",
    "    csfont = {'fontname':'Times New Roman'}\n",
    "    range_future = len(prediction)\n",
    "    plt.plot(np.arange(range_future), np.array(y_test),  label='Test data',linewidth=2)\n",
    "    plt.plot(np.arange(range_future),np.array(prediction),label='Prediction',linewidth=2)\n",
    "    plt.legend(loc='upper left',fontsize=14)\n",
    "    plt.xlabel('Time (second)',fontsize=16,**csfont)\n",
    "    plt.ylabel('PM2.5 (Âµg/m3)',fontsize=16,**csfont)\n",
    "    \n",
    "plot_future(prediction_result, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
